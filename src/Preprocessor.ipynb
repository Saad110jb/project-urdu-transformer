{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a57158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/preprocess.py\n",
    "\"\"\"\n",
    "Transcription and preprocessing pipeline:\n",
    "1. Transcribe audio files to Urdu text (uses whisper if available).\n",
    "2. Normalize Urdu text (strip diacritics, normalize alef/yeh forms, remove extra spaces).\n",
    "3. Train SentencePiece model on all transcriptions to build vocabulary.\n",
    "4. Produce train/val/test splits and save tokenized files.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Optional whisper import. If not available, instruct students to install or use other ASR.\n",
    "try:\n",
    "    import whisper\n",
    "    WHISPER_AVAILABLE = True\n",
    "except Exception:\n",
    "    WHISPER_AVAILABLE = False\n",
    "\n",
    "AUDIO_DIR = \"data/audio\"\n",
    "TRANSCRIPT_CSV = \"data/transcripts.csv\"\n",
    "SP_MODEL_PREFIX = \"data/ur_tokenizer\"\n",
    "SP_VOCAB_SIZE = 8000\n",
    "\n",
    "# ---------- 1) Transcription ----------\n",
    "def transcribe_with_whisper(model_size=\"small\"):\n",
    "    assert WHISPER_AVAILABLE, \"whisper is not installed. pip install openai-whisper\"\n",
    "    model = whisper.load_model(model_size)  # 'small' is a good compromise\n",
    "    rows = []\n",
    "    files = sorted(glob.glob(os.path.join(AUDIO_DIR, \"*.*\")))\n",
    "    for fpath in tqdm(files, desc=\"Transcribing audio\"):\n",
    "        try:\n",
    "            result = model.transcribe(fpath, language=\"ur\", task=\"transcribe\")\n",
    "            text = result.get(\"text\", \"\").strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error transcribing {fpath}: {e}\")\n",
    "            text = \"\"\n",
    "        rows.append({\"filename\": os.path.basename(fpath), \"transcript\": text})\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(TRANSCRIPT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved transcripts to {TRANSCRIPT_CSV}\")\n",
    "\n",
    "# ---------- 2) Normalization ----------\n",
    "# Simple Urdu normalization: remove harakat/diacritics and map multiple alef/yeh forms\n",
    "URDU_DIACRITICS = [\n",
    "    '\\u0610', '\\u0611', '\\u0612', '\\u0613', '\\u0614', '\\u0615', '\\u0616', '\\u0617',\n",
    "    '\\u0618', '\\u0619', '\\u061A', '\\u064B', '\\u064C', '\\u064D', '\\u064E', '\\u064F',\n",
    "    '\\u0650', '\\u0651', '\\u0652', '\\u0653', '\\u0654', '\\u0655'\n",
    "]\n",
    "DIACRITIC_PATTERN = re.compile(\"|\".join(URDU_DIACRITICS))\n",
    "\n",
    "def normalize_urdu(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = DIACRITIC_PATTERN.sub(\"\", text)   # strip diacritics\n",
    "    # normalize alef variants to simple alef (ا)\n",
    "    text = re.sub(r'[أإآٱ]', 'ا', text)\n",
    "    # normalize yeh variants to ی (U+06CC) commonly used in Urdu\n",
    "    text = re.sub(r'[يى]', 'ی', text)\n",
    "    # optionally normalize hamza/ta marbuta etc if needed\n",
    "    # collapse multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def normalize_all(transcript_csv=TRANSCRIPT_CSV):\n",
    "    df = pd.read_csv(transcript_csv, encoding=\"utf-8\")\n",
    "    df['normalized'] = df['transcript'].fillna(\"\").astype(str).apply(normalize_urdu)\n",
    "    df.to_csv(transcript_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Normalized transcripts saved to {transcript_csv}\")\n",
    "\n",
    "# ---------- 3) SentencePiece training ----------\n",
    "def train_sentencepiece(input_csv=TRANSCRIPT_CSV, prefix=SP_MODEL_PREFIX, vocab_size=SP_VOCAB_SIZE):\n",
    "    df = pd.read_csv(input_csv, encoding=\"utf-8\")\n",
    "    all_text = \"\\n\".join(df['normalized'].dropna().astype(str).tolist())\n",
    "    temp_text_path = \"data/all_transcripts.txt\"\n",
    "    Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "    with open(temp_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(all_text)\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=temp_text_path,\n",
    "        model_prefix=prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        model_type='unigram',  # unigram often works well for morphologically rich languages\n",
    "        user_defined_symbols=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "    )\n",
    "    print(\"Trained SentencePiece model at\", prefix + \".model\")\n",
    "\n",
    "# ---------- 4) Split dataset ----------\n",
    "def create_splits(transcript_csv=TRANSCRIPT_CSV, out_dir=\"data/splits\", train_frac=0.8, val_frac=0.1, test_frac=0.1, seed=42):\n",
    "    df = pd.read_csv(transcript_csv, encoding=\"utf-8\")\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    n_train = int(n * train_frac)\n",
    "    n_val = int(n * val_frac)\n",
    "    train = df.iloc[:n_train]\n",
    "    val = df.iloc[n_train:n_train+n_val]\n",
    "    test = df.iloc[n_train+n_val:]\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    train.to_csv(os.path.join(out_dir, \"train.csv\"), index=False, encoding=\"utf-8\")\n",
    "    val.to_csv(os.path.join(out_dir, \"val.csv\"), index=False, encoding=\"utf-8\")\n",
    "    test.to_csv(os.path.join(out_dir, \"test.csv\"), index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved splits to {out_dir} (train/val/test sizes: {len(train)}/{len(val)}/{len(test)})\")\n",
    "\n",
    "# ---------- main ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Transcribe\n",
    "    if WHISPER_AVAILABLE:\n",
    "        transcribe_with_whisper(model_size=\"small\")  # change as necessary\n",
    "    else:\n",
    "        print(\"Whisper not available. Please transcribe audio by other means and create data/transcripts.csv\")\n",
    "\n",
    "    # 2. Normalize text\n",
    "    normalize_all()\n",
    "\n",
    "    # 3. Train SentencePiece\n",
    "    train_sentencepiece()\n",
    "\n",
    "    # 4. Create splits\n",
    "    create_splits()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
